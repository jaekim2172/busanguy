{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "02-logistic-regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekim2172/busanguy/blob/master/02_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LuyXJXkGRjh2",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Linear regression's purpose is minimize the RSS(Residual Sum of Squares). But linear regression can't apply on specific situation. For example, If the dependent value($y$) is non-numerical(e.g. binary categorical), regression result must be weird. So, we have to re-define of this problem as solving probability of binary class.  \n",
        "\n",
        "----\n",
        "\n",
        "### Summary\n",
        "- The logistic(sigmoid) function's output range always $ 0< f(x) < 1 $.\n",
        "- It satisfies probability density function's rule.\n",
        "- So, we can use this for binary classification.\n",
        "\n",
        "$$ f(x) = \\frac{1}{1+e^{-x}} $$\n",
        "\n",
        "\n",
        "----\n",
        "### Logistic Regression for Binary Classification\n",
        "\n",
        "- Now we can modeling $ P(y=1|X) $ when $ y = \\{1,0\\} $. \n",
        "- Follow equation is logistic regression's equation.\n",
        "\n",
        "$$ P(X) = \\frac{1}{1+e^{-(b_0 + WX)}} $$\n",
        "\n",
        "----\n",
        "### How to Derive\n",
        "- It derived from the concept of $odds$.\n",
        "- Below equation means(odds means) that if the P(A) is increases, odds(승산) also increases.\n",
        "\n",
        "$$ odds = \\frac{P(A)}{P(A^c)} = \\frac{P(A)}{1-P(A)}$$\n",
        "\n",
        "- And we have below equation. Because we have to solve this problem as probability of specific class. But left equation's range is $ 0 $ to $ 1 $ and right equation's range is $ -\\infty $ to $ \\infty $ \n",
        "\n",
        "$$ P(y=1|X) = b_0 + WX $$\n",
        "\n",
        "- Let's turn this problem into a odds problem.\n",
        "\n",
        "$$ \\frac{P(y=1|X)}{1 - P(y=1|X)} = b_0 + WX $$\n",
        "\n",
        "- But left equation's range is still $ 0 $ to $ \\infty $. So, we have use $log$.\n",
        "\n",
        "$$ ln(\\frac{P(y=1|X)}{1 - P(y=1|X)}) = b_0 + WX $$\n",
        "\n",
        "- Now, the range of both equation is $ -\\infty $ to $ \\infty $ \n",
        "- Below equations are the process of solving equation line by line.\n",
        "\n",
        "$$ \\frac{P(y=1|X)}{1 - P(y=1|X)})= e^{b_0 + WX} $$\n",
        "\n",
        "$$ \\frac{P(y=1|X)}{1 - P(y=1|X)})= e^{b_0 + WX} $$\n",
        "\n",
        "$$ P(y=1|X) = \\frac{e^{b_0 + WX}}{1+e^{b_0 + WX}} $$\n",
        "\n",
        "$$ \\therefore P(y=1|X) = \\frac{1}{1+e^{-(b_0 + WX)}} $$\n",
        "\n",
        "----\n",
        "### Decision Boundary\n",
        "\n",
        "Generally, The decision boundary of binary classification is 0.5. And logistic regression also like this. Now we follow the process of derive decision boundary.\n",
        "\n",
        "- Make below equation to find decision boundary(Criteria that can be classified as 1).\n",
        "\n",
        "$$ P(y=1|X) > P(y=0|X) $$\n",
        "\n",
        "- And, the below equations are the process of solving equation line by line.\n",
        "\n",
        "> - $$ P(y=1|X) > 1 - P(y=1|X) $$\n",
        "> - $$ P(y=1|X) > 0.5 $$\n",
        "> - $$ \\therefore P(y=1|X) = \\frac{1}{1+e^{-(b_0 + WX)}} > 0.5 $$\n",
        "\n",
        "or\n",
        "\n",
        "> - $$ P(y=1|X) > 1 - P(y=1|X) $$\n",
        "> - $$ \\frac{P(y=1|X)}{1-P(y=1|X)} > 1$$\n",
        "> - $$ ln(\\frac{P(y=1|X)}{1-P(y=1|X)}) > 0$$\n",
        "> - $$ \\therefore b_0 + WX > 0 $$\n",
        "\n",
        "----\n",
        "### Logistic Regression with MLE\n",
        "\n",
        "Now, we have to find cost function for gradient descent. Because of logistic model's non-linearity, we can't use deterministic model. Likelihood is also calculated from PDF functions but by calculating the joint probabilities of data points from a particular PDF function. In this case, instead of normal distribution, Bernoulli distribution is our proper PDF.\n",
        "\n",
        "$$ L(parametes | data) = \\prod_{i=1}^{m} f(data_i | parameters) $$\n",
        "\n",
        "- Bernoulli trial : bernoulli trial means that the results of experiment(=event) are binary case.\n",
        "- For example, the binary probability of event is below.\n",
        "\n",
        "$$ P(y=1) = p, P(y=0) = 1-p $$\n",
        "\n",
        "$$ P(Y=y_i) = p^{y_{i}} (1-p)^{1-y_i}, (y=1,0) $$\n",
        "\n",
        "- And Bernoulli distribution is like that.\n",
        "\n",
        "$$ Bern(y; p) = p^{y} (1-p)^{1-y} $$\n",
        "\n",
        "- Now we can summary some equations. ($ Note, f(x)=logistic(x) $)\n",
        "\n",
        "$$ L = \\prod_i p^{y_{i}} (1-p)^{1-y_i}$$\n",
        "\n",
        "$$ L = \\prod_i f(b_0 + XW)^{y_{i}} (1-f(b_0 + XW))^{1-y_i}$$\n",
        "\n",
        "$$ ln(L) = \\sum_i y_i ln(f(b_0 + XW)) + \\sum_i (1-y_i)ln(1-f(b_0 + XW)))$$\n",
        "\n",
        "- So, the log-likelihood of logistic regression model is defined. And this is cost function of logistic regression.\n",
        "\n",
        "----\n",
        "### Gradient Descent\n",
        "\n",
        "- After long long long long formula expansion [(link)](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated), we can find the gradient of cost function (log-likelihood)  \n",
        "\n",
        "$$ \\frac{d}{dW}ln(L) = \\frac{1}{m} \\sum_{i=1}^{m}(f(WX) - y_i)x_j $$\n",
        "\n",
        "$$ w_j = w_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m}(f(WX) - y_i)x_j $$\n",
        "\n",
        "----\n",
        "#### references\n",
        "- https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
        "- https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/\n",
        "- https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated"
      ]
    }
  ]
}